package news

import (
	"crypto/sha1"
	"encoding/hex"
	"fmt"
	"log"
	"net/url"
	"regexp"
	"sort"
	"strings"
	"time"
	"unicode"
	"unicode/utf8"

	"github.com/deusflow/News/internal/gemini"
	"github.com/deusflow/News/internal/metrics"
	"github.com/deusflow/News/internal/rss"
	"github.com/deusflow/News/internal/scraper"
	"github.com/deusflow/News/internal/translate" // –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º—ã –ø–µ—Ä–µ–≤–æ–¥–æ–≤
)

// News represents a single news item enriched by AI summaries with image support.
type News struct {
	Title     string
	Content   string
	Link      string
	Published time.Time
	Category  string
	Score     int

	SourceName       string
	SourceLang       string
	SourceCategories []string

	Summary          string // Original language summary (or detected)
	SummaryDanish    string // Danish version of summary
	SummaryUkrainian string // Ukrainian version of summary
	TitleUkrainian   string // Ukrainian title (translated from Title)

	// Image support - –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
	ImageURL string // URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏
	ImageAlt string // –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
}

// Extra boost keywords for refugee/visa related stories to increase priority
var refugeeBoostKeywords = []string{
	"refugee",
	"viborg",
	"flygtning",
	"refugee visa",
	"temporary protection",
	"asylum",
	"asylum support",
	"asylum application",
	"asylum application form",
	"asylum application form ukraine",
	"asylum application form denmark",
	"families",
	"family",
}

var visaBoostKeywords = []string{
	"visum",
	"visumforl√¶ngelse",
	"opholdstilladelse",
	"blive i EU",
}

// –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ / "—É–∫—Ä–∞–∏–Ω—Å–∫–∏–µ" —Ç–µ—Ä–º–∏–Ω—ã (–ø—Ä–æ —Å–∞–º—É –£–∫—Ä–∞–∏–Ω—É –∏ —É–∫—Ä–∞–∏–Ω—Ü–µ–≤)
var ukraineGeoKeywords = []string{
	"ukraine", "ukraina", "ukrainer", "ukrainsk", "ukrainere", "ukrainske",
	"ukrainske familier", "ukrainske i danmark", "ukrainere i danmark",
	"ukrainsk diaspora", "flygtninge fra ukraine",
}

var denmarkKeywords = []string{
	"danmark", "danske", "k√∏benhavn", "aarhus", "aalborg", "viborg",
	"region", "kommune", "borgere", "lov", "politik", "√∏konomi",
	"visum", "opholdstilladelse", "asyl", "integration", "arbejde", "bolig",
	"udl√¶ndinge",
}

var conflictKeywords = []string{
	"krig", "krigen", "putin", "zelensky", "invasion", "bomb", "missil", "russisk", "war", "invasion",
}

// –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ / –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ / —Å—Ç–∞—Ä—Ç–∞–ø—ã / –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
var techKeywords = []string{
	"teknologi", "innovation", "startup", "forskning", "research", "patent",
	"robot", "software", "hardware", "IT", "cloud", "cyber", "data",
	"machine learning", "deep learning", "artificial intelligence", "AI", "maskinl√¶ring", "LLM",
}

// –ò—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ AI-—Ç–µ—Ä–º–∏–Ω—ã (—á—Ç–æ–±—ã —Ç–æ—á–Ω–æ –ø–æ–π–º–∞—Ç—å –ò–ò-–Ω–æ–≤–æ—Å—Ç–∏)
var aiKeywords = []string{
	"ai", "artificial intelligence", "maskinl√¶ring", "neuralt netv√¶rk", "large language model", "llm",
}

// –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ / —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–º—ã
var medicalKeywords = []string{
	"l√¶gemidler", "medicin", "vaccine", "klinisk fors√∏g", "pharma", "biotek", "behandling", "treatment",
}

// Words to exclude (not important topics)
var excludeKeywords = []string{
	"vejr",
	"musik",
	"film",
	"kendis",
	"fodboldresultat",
	"sportsresultat",
	"tv-program",
	"horoskop",
	"madopskrift",
}

// –ï–≤—Ä–æ–ø–∞ / –µ–≤—Ä–æ–ø–µ–π—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (—à–∏—Ä–µ —á–µ–º –î–∞–Ω–∏—è)
var europeKeywords = []string{
	"europa", "eu", "european", "eu-lande", "europeisk",
}

// –¢–µ–º–∞—Ç–∏–∫–∏ –¥–ª—è –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤ –∏ —Ä–æ–¥–∏—Ç–µ–ª–µ–π
var youthKeywords = []string{
	"ungdom", "teenager", "unge", "skole", "gymnasium", "uddannelse", "studerende",
	"fritid", "sport", "gaming", "esport", "social media", "mobil", "app",
	"musik", "festival", "koncert", "streaming", "youtube", "tiktok", "instagram",
	"snapchat", "discord", "twitch", "netflix", "spotify", "podcast",
	"mode", "influencer", "blogger", "vlogger", "content creator",
	"mental sundhed", "stress", "angst", "selvv√¶rd", "mobning", "cybermobning",
	"k√¶reste", "venskab", "dating", "ungdomskultur", "trend", "viral",
	"uddannelsesvalg", "studievejledning", "efterskole", "gap year",
	"job", "praktikplads", "sommerjob", "ungdomsarbejde", "cv",
}

var parentKeywords = []string{
	"for√¶ldre", "b√∏rn", "familie", "dagpleje", "b√∏rnehave", "skole", "mor", "far",
	"graviditet", "f√∏dsel", "baby", "sm√•b√∏rn", "teenager", "opdragelse", "familie√∏konomi",
	"b√∏rnepenge", "orlov", "barsel", "familieydelse", "SFO", "fritidsordning",
	"m√∏dregruppe", "f√¶dregruppe", "for√¶ldrem√∏de", "for√¶ldreinddragelse",
	"b√∏rns udvikling", "motorik", "sprog", "l√¶sning", "matematik",
	"allergi", "astma", "vaccination", "sundhedspleje", "b√∏rnel√¶ge",
	"skilsmisse", "samv√¶r", "b√∏rnebidrag", "for√¶ldremyndighed",
	"digital opdragelse", "sk√¶rmtid", "online sikkerhed", "cybersikkerhed",
	"bullying", "mobning", "skolev√¶gring", "s√¶rlige behov", "inklusion",
	"familieaktiviteter", "ferie", "b√∏rnevenlig", "legeplads", "zoo", "museum",
	"boligs√∏gning", "b√∏rnevenlig bolig", "sikkerhed hjemme", "babyproofing",
}

var culturalKeywords = []string{
	"kultur", "museum", "teater", "opera", "kunst", "udstilling", "galleri",
	"litteratur", "bog", "forfatter", "bibliotek", "kulturel", "traditions",
	"folkefest", "festival", "kulturnat", "kunstmuseum", "kulturhus",
	"dansk kultur", "historie", "arv", "traditioner", "kulturformidling",
	"scene", "skuespil", "ballet", "koncert", "klassisk musik", "jazz",
	"film", "documentary", "kortfilm", "filminstrukt√∏r", "dansk film",
	"design", "arkitektur", "m√∏bler", "dansk design", "designmuseum",
}

var sportsKeywords = []string{
	"sport", "fodbold", "h√•ndbold", "cykling", "sv√∏mning", "atletik", "fitness",
	"idr√¶t", "konkurrence", "mesterskab", "olympiske", "VM", "EM",
	"badminton", "tennis", "basketball", "volleyball", "gymnastik",
	"l√∏b", "marathon", "triathlon", "styrketr√¶ning", "crossfit",
	"b√∏rnesport", "ungdomsidr√¶t", "idr√¶tsforening", "klub", "hold",
	"sundhed", "motion", "aktiv", "tr√¶ning", "coaching", "instrukt√∏r",
	"parasport", "handicapidr√¶t", "inklusion i sport", "tilg√¶ngelighed",
}

// improved containsAny: distinguishes phrases and short words (avoids "ai" matching "said")
func containsAny(text string, keywords []string) bool {
	text = strings.ToLower(text)

	for _, k := range keywords {
		k = strings.ToLower(strings.TrimSpace(k))
		if k == "" {
			continue
		}

		// If keyword is a phrase (contains space) -> substring match
		if strings.Contains(k, " ") {
			if strings.Contains(text, k) {
				return true
			}
			continue
		}

		// Short tokens (<=3) -> whole word match using word boundary regexp
		if len(k) <= 3 {
			// Use regexp.QuoteMeta to avoid accidental meta-chars in keyword
			re := regexp.MustCompile(`\b` + regexp.QuoteMeta(k) + `\b`)
			if re.MatchString(text) {
				return true
			}
			continue
		}

		// Otherwise, simple substring is fine
		if strings.Contains(text, k) {
			return true
		}
	}
	return false
}

// makeNewsKey generates a hash key from title and description for deduplication
func makeNewsKey(title, description string) string {
	h := sha1.New()
	h.Write([]byte(strings.ToLower(title + description)))
	return hex.EncodeToString(h.Sum(nil))
}

// makeSimilarityKey creates a more lenient key for detecting similar news
// makeSimilarityKey - –º–µ–Ω–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è.
// –õ–æ–≥–∏–∫–∞:
// 1) –ë–µ—Ä—ë–º host –∏–∑ item.Link (–µ—Å–ª–∏ –µ—Å—Ç—å) ‚Äî —á—Ç–æ–±—ã –∫–ª—é—á –±—ã–ª —Å–ø–µ—Ü–∏—Ñ–∏—á–µ–Ω –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
// 2) –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫: lowercase, —É–±–∏–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, —É–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞.
// 3) –û—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–µ N –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 6) ‚Äî —á—Ç–æ–±—ã –Ω–µ —Å–∫–ª–µ–∏–≤–∞—Ç—å —Å–ª–∏—à–∫–æ–º —Ä–∞–∑–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏.
// 4) –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ä–µ–∑ (truncate –ø–æ –æ–∫–Ω—É –≤ hours, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 6—á).
// –†–µ–∑—É–ª—å—Ç–∞—Ç: host|topWords|windowUnix
func makeSimilarityKey(item *rss.FeedItem) string {
	// –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å
	const (
		windowHours = 6 // –æ–∫–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –¥–µ–¥—É–ø–∞ (–º–µ–Ω—å—à–µ -> –º–µ–Ω—å—à–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç–∏)
		maxWords    = 6 // —Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ –æ—Å—Ç–∞–≤–∏—Ç—å
	)

	// Helper: –ø–æ–ª—É—á–∏—Ç—å host –∏–∑ —Å—Å—ã–ª–∫–∏
	getHost := func(link string) string {
		if link == "" {
			return "unknown"
		}
		u, err := url.Parse(link)
		if err != nil || u.Host == "" {
			// –∏–Ω–æ–≥–¥–∞ –≤ feed –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ª–∏–Ω–∫ –∏–ª–∏ –ø—É—Å—Ç–æ–π
			return "unknown"
		}
		return strings.ToLower(u.Host)
	}

	// Helper: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ ‚Äî —É–±—Ä–∞—Ç—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, multiple spaces, lower
	normalize := func(s string) string {
		s = strings.ToLower(s)
		// —É–¥–∞–ª–∏—Ç—å HTML-—Ç–µ–≥–∏ –µ—Å–ª–∏ –≤–¥—Ä—É–≥
		reTags := regexp.MustCompile(`<[^>]*>`)
		s = reTags.ReplaceAllString(s, " ")

		// –û—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã (Unicode-aware)
		var b []rune
		b = make([]rune, 0, len(s))
		for _, r := range s {
			if unicode.IsLetter(r) || unicode.IsNumber(r) || unicode.IsSpace(r) {
				b = append(b, r)
			} else {
				// –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –ø—Ä–æ–±–µ–ª, —á—Ç–æ–±—ã —Ä–∞–∑–¥–µ–ª—è—Ç—å —Å–ª–æ–≤–∞
				b = append(b, ' ')
			}
		}
		out := strings.Join(strings.Fields(string(b)), " ")
		return out
	}

	// –ù–µ–±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä —Å—Ç–æ–ø-—Å–ª–æ–≤ ‚Äî —Ä–∞—Å—à–∏—Ä—è–π –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (–¥–∞—Ç—Å–∫–∏–π/–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
	stopWords := map[string]bool{
		"a": true, "an": true, "the": true, "og": true, "i": true, "p√•": true,
		"til": true, "af": true, "med": true, "for": true, "er": true, "der": true,
		"om": true, "en": true, "et": true, "ikke": true,
	}

	// –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç: title + short description
	text := strings.TrimSpace(item.Title + " " + item.Description)
	norm := normalize(text)
	words := strings.Fields(norm)

	// –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ ¬´–∑–Ω–∞—á–∏–º—ã–µ¬ª —Å–ª–æ–≤–∞
	significant := make([]string, 0, len(words))
	for _, w := range words {
		if len(significant) >= maxWords {
			break
		}
		if stopWords[w] {
			continue
		}
		// –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (<=2)
		if len(w) <= 2 {
			continue
		}
		significant = append(significant, w)
	}
	// –ï—Å–ª–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ ‚Äî –≤–æ–∑—å–º—ë–º –ø–µ—Ä–≤—ã–µ maxWords –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ (–±–µ–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
	if len(significant) == 0 && len(words) > 0 {
		for i := 0; i < len(words) && i < maxWords; i++ {
			significant = append(significant, words[i])
		}
	}

	// –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ä–µ–∑: –∏—Å–ø–æ–ª—å–∑—É–µ–º PublishedParsed –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ —Ç–µ–∫—É—â–∏–π —á–∞—Å
	var t time.Time
	if item.PublishedParsed != nil {
		t = *item.PublishedParsed
	} else if item.Published != "" {
		// –ø–æ–ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å Published (–±–µ–∑ –≥–∞—Ä–∞–Ω—Ç–∏–π) ‚Äî –±–µ–∑–æ–ø–∞—Å–Ω—ã–π fallback
		if parsed, err := time.Parse(time.RFC1123Z, item.Published); err == nil {
			t = parsed
		} else if parsed2, err2 := time.Parse(time.RFC1123, item.Published); err2 == nil {
			t = parsed2
		} else {
			t = time.Now()
		}
	} else {
		t = time.Now()
	}
	// –û–±—Ä–µ–∑–∞–µ–º –≤—Ä–µ–º—è –¥–æ –Ω–∞—á–∞–ª–∞ –æ–∫–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 6—á)
	windowStart := t.Truncate(time.Duration(windowHours) * time.Hour).Unix()

	host := getHost(item.Link)

	// –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª—é—á
	key := fmt.Sprintf("%s|%s|%d", host, strings.Join(significant, "_"), windowStart)
	return key
}

// calculateNewsScore - –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏–∏
func calculateNewsScore(item *rss.FeedItem) (string, int) {
	text := strings.ToLower(item.Title + " " + item.Description)

	// –ë—ã—Å—Ç—Ä–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
	if containsAny(text, excludeKeywords) {
		return "", 0
	}

	// –§–ª–∞–≥–∏
	hasDenmark := containsAny(text, denmarkKeywords)
	hasUkraineGeo := containsAny(text, ukraineGeoKeywords)
	hasEurope := containsAny(text, europeKeywords)
	hasTech := containsAny(text, techKeywords)
	hasMedical := containsAny(text, medicalKeywords)
	hasConflict := containsAny(text, conflictKeywords)
	hasRefugeeBoost := containsAny(text, refugeeBoostKeywords)
	hasVisaBoost := containsAny(text, visaBoostKeywords)
	hasYouth := containsAny(text, youthKeywords)
	hasParent := containsAny(text, parentKeywords)
	hasCultural := containsAny(text, culturalKeywords)
	hasSports := containsAny(text, sportsKeywords)

	ctxLocal := hasDenmark || hasUkraineGeo || hasEurope

	// –ï—Å–ª–∏ —ç—Ç–æ —Ç–æ–ª—å–∫–æ "–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–µ" —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –≤–æ–π–Ω—ã/–ü—É—Ç–∏–Ω –±–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
	if hasConflict && !ctxLocal {
		return "", 0
	}

	// –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
	var category string
	score := 0

	// 1) –ù–æ–≤–æ—Å—Ç–∏ –ø—Ä–æ —É–∫—Ä–∞–∏–Ω—Ü–µ–≤ / –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∂–µ–Ω—Ü–µ–≤ / –≤–∏–∑—ã ‚Äî –≤—ã—Å–æ–∫–∞—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç—å
	if hasUkraineGeo || hasRefugeeBoost || hasVisaBoost {
		category = "ukraine"
		score = 70
		if hasDenmark {
			score += 15
		}
		if hasEurope {
			score += 5
		}
		if hasConflict && !(hasRefugeeBoost || hasVisaBoost || hasDenmark) {
			score -= 15
		}
		if hasTech {
			score += 10
		}
		if hasMedical {
			score += 10
		}
		return category, score
	}

	// 2) –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏/–º–µ–¥–∏—Ü–∏–Ω–∞ ‚Äî —Ç—Ä–µ–±—É–µ–º –≥–µ–æ-–∫–æ–Ω—Ç–µ–∫—Å—Ç
	if hasTech || hasMedical {
		if !ctxLocal {
			return "", 0
		}
		if hasMedical {
			category = "health"
		} else {
			category = "tech"
		}
		score = 80
		if containsAny(text, aiKeywords) {
			score += 10
		}
		if hasDenmark {
			score += 10
		}
		if hasEurope {
			score += 5
		}
		return category, score
	}

	// 3) –°–µ–º—å—è/—Ä–æ–¥–∏—Ç–µ–ª–∏ (–¥–æ –æ–±—â–µ–≥–æ –¥–∞—Ç—Å–∫–æ–≥–æ –±–ª–æ–∫–∞, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ unreachable –±–æ–Ω—É—Å–æ–≤)
	if hasParent && ctxLocal {
		category = "family"
		score = 55
		if hasDenmark {
			score += 10
		}
		return category, score
	}

	// 4) –ú–æ–ª–æ–¥–µ–∂–Ω—ã–µ —Ç–µ–º—ã
	if hasYouth && ctxLocal {
		category = "youth"
		score = 50
		if hasDenmark {
			score += 8
		}
		return category, score
	}

	// 5) –ö—É–ª—å—Ç—É—Ä–∞
	if hasCultural && ctxLocal {
		category = "culture"
		score = 35
		if hasDenmark {
			score += 10
		}
		return category, score
	}

	// 6) –°–ø–æ—Ä—Ç
	if hasSports && ctxLocal {
		category = "sports"
		score = 30
		if hasDenmark {
			score += 8
		}
		return category, score
	}

	// 7) –û–±—â–∏–µ –¥–∞—Ç—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
	if hasDenmark {
		category = "denmark"
		score = 40
		if containsAny(text, []string{"politik", "regering", "√∏konomi", "minister"}) {
			score += 15
		}
		return category, score
	}

	// 8) –û–±—â–∏–µ –µ–≤—Ä–æ–ø–µ–π—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ (–±–µ–∑ –¥–∞—Ç—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
	if hasEurope {
		category = "europe"
		score = 25
		return category, score
	}

	// 9) –ß–∏—Å—Ç–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
	if hasConflict {
		category = "conflict"
		score = 15
		return category, score
	}

	// 10) –û–±—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
	if containsAny(text, []string{"√∏konomi", "business", "marked", "aktier", "bank"}) {
		category = "economy"
		score = 20
	} else if containsAny(text, []string{"milj√∏", "klima", "climate", "environment", "gr√∏n"}) {
		category = "environment"
		score = 25
	} else if containsAny(text, []string{"uddannelse", "education", "universitet"}) {
		category = "education"
		score = 22
	} else if containsAny(text, []string{"europa", "european", "eu"}) {
		category = "general"
		score = 10
	}

	if category == "" || score == 0 {
		return "", 0
	}

	return category, score
}

// Gemini client injection
var aiClient *gemini.Client

// SetGeminiClient sets the Gemini client for translation and summarization
func SetGeminiClient(c *gemini.Client) {
	aiClient = c
}

// FilterAndTranslate: —Ñ–∏–ª—å—Ç—Ä + —Å–∫—Ä–∞–ø–∏–Ω–≥ + —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏—è Gemini + –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ —Å–∞–º–º–∞—Ä–∏.
func FilterAndTranslate(items []*rss.FeedItem) ([]News, error) {
	return FilterAndTranslateWithOptions(items, Options{})
}

// Options controls filtering and selection behavior.
type Options struct {
	Limit             int           // how many items to return
	MaxAge            time.Duration // discard items older than this
	PerSource         int           // cap per source in final list
	PerCategory       int           // cap per category in final list
	MaxGeminiRequests int           // maximum Gemini requests allowed (0 = unlimited)
}

// FilterAndTranslateWithOptions performs filtering and summarization using provided options.
func FilterAndTranslateWithOptions(items []*rss.FeedItem, opts Options) ([]News, error) {
	startTime := time.Now()
	defer func() {
		metrics.Global.RecordProcessingTime(time.Since(startTime))
		metrics.Global.SetLastRun()
	}()

	if aiClient == nil {
		return nil, fmt.Errorf("gemini client not initialized; call news.SetGeminiClient")
	}
	log.Println("[Gemini] Starting filter + scrape + summarize pipeline (WithOptions)")

	// defaults
	if opts.Limit <= 0 {
		opts.Limit = 8
	}
	if opts.MaxAge <= 0 {
		opts.MaxAge = 24 * time.Hour
	}
	if opts.PerSource <= 0 {
		opts.PerSource = 2
	}
	if opts.PerCategory <= 0 {
		opts.PerCategory = 2
	}

	seenLinks := map[string]struct{}{}
	seenContent := map[string]struct{}{}
	seenSimilar := map[string]struct{}{}
	var seenTitles []string
	var candidates []News

	log.Printf("–ù–∞—á–∏–Ω–∞–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∏–∑ %d –Ω–æ–≤–æ—Å—Ç–µ–π (maxAge=%s)", len(items), opts.MaxAge)

	for _, item := range items {
		metrics.Global.IncrementNewsProcessed()

		// –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ –≤–æ–∑—Ä–∞—Å—Ç—É
		if item.PublishedParsed != nil && time.Since(*item.PublishedParsed) > opts.MaxAge {
			continue
		}

		// –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å—Å—ã–ª–∫–µ
		normalizedLink := normalizeURL(item.Link)
		if _, dup := seenLinks[normalizedLink]; dup {
			metrics.Global.IncrementDuplicatesFiltered()
			continue
		}
		seenLinks[normalizedLink] = struct{}{}

		// –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é (–∑–∞–≥–æ–ª–æ–≤–æ–∫ + –æ–ø–∏—Å–∞–Ω–∏–µ)
		key := makeNewsKey(item.Title, item.Description)
		if _, dup := seenContent[key]; dup {
			metrics.Global.IncrementDuplicatesFiltered()
			continue
		}
		seenContent[key] = struct{}{}

		// –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–±–æ–ª–µ–µ –º—è–≥–∫–∞—è)
		similarKey := makeSimilarityKey(item)
		if _, dup := seenSimilar[similarKey]; dup {
			metrics.Global.IncrementDuplicatesFiltered()
			continue
		}
		seenSimilar[similarKey] = struct{}{}

		// –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏
		skipSimilar := false
		for _, existingTitle := range seenTitles {
			if isSimilarTitle(item.Title, existingTitle) {
				metrics.Global.IncrementDuplicatesFiltered()
				skipSimilar = true
				break
			}
		}
		if skipSimilar {
			continue
		}

		// –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∏ —Å–∫–æ—Ä
		category, score := calculateNewsScore(item)
		if score == 0 {
			continue
		}

		published := time.Now()
		if item.PublishedParsed != nil {
			published = *item.PublishedParsed
		}

		sourceName, sourceLang := "", ""
		var sourceCategories []string
		if item.Source != nil {
			sourceName = item.Source.Name
			sourceLang = item.Source.Lang
			sourceCategories = item.Source.Categories
		}

		candidates = append(candidates, News{
			Title:            item.Title,
			Content:          item.Description,
			Link:             item.Link,
			Published:        published,
			Category:         category,
			Score:            score,
			SourceName:       sourceName,
			SourceLang:       sourceLang,
			SourceCategories: sourceCategories,
			// –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ RSS –∏–ª–∏ –∏–∑ —Å—Å—ã–ª–∫–∏
			ImageURL: extractImageURL(item),
			ImageAlt: item.Title, // –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ç–µ–∫—Å—Ç
		})

		seenTitles = append(seenTitles, item.Title)
	}

	// –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–∫–æ—Ä, –∑–∞—Ç–µ–º –Ω–æ–≤–∏–∑–Ω–∞
	sort.Slice(candidates, func(i, j int) bool {
		if candidates[i].Score != candidates[j].Score {
			return candidates[i].Score > candidates[j].Score
		}
		return candidates[i].Published.After(candidates[j].Published)
	})

	if len(candidates) == 0 {
		return nil, nil
	}

	// –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: –±–µ—Ä—ë–º –±–æ–ª—å—à–µ –ø—É–ª–∞, —á–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ª–∏–º–∏—Ç, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å –ø–æ–∫—Ä—ã—Ç–∏–µ
	pool := opts.Limit * 4
	if pool > len(candidates) {
		pool = len(candidates)
	}
	diverseCandidates := selectDiverse(candidates[:pool], opts.Limit, opts.PerSource, opts.PerCategory)

	newsLimit := opts.Limit
	if len(diverseCandidates) < newsLimit {
		newsLimit = len(diverseCandidates)
	}

	urls := make([]string, newsLimit)
	for i := 0; i < newsLimit; i++ {
		urls[i] = diverseCandidates[i].Link
	}

	log.Printf("–ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç %d —Å—Ç–∞—Ç–µ–π...", newsLimit)
	fullArticles := scraper.ExtractArticlesInBackground(urls)

	res := make([]News, 0, newsLimit)
	geminiRequests := 0
	for i := 0; i < newsLimit; i++ {
		n := diverseCandidates[i]
		log.Printf("Getting full content of article %d/%d: %s", i+1, newsLimit, n.Link)

		if fa, ok := fullArticles[n.Link]; ok && len(fa.Content) > 200 {
			n.Content = fa.Content
			log.Printf("‚úÖ Got content (%d chars)", len(fa.Content))
		} else {
			log.Printf("‚ö†Ô∏è Using short description for: %s", n.Title)
		}

		// –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫
		sourceLang := "da" // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–∞—Ç—Å–∫–∏–π
		if n.SourceLang != "" {
			sourceLang = n.SourceLang
		}

		// –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã Gemini
		if opts.MaxGeminiRequests > 0 && geminiRequests >= opts.MaxGeminiRequests {
			log.Printf("‚ö†Ô∏è Gemini requests limit exceeded, using fallback AI services")

			// –ö—Ä–∞—Ç–∫–∞—è —Å—É—Ç—å –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º —è–∑—ã–∫–µ (–¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è)
			n.Summary = fallbackSummary(n.Content)

			// –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ AI –¥–ª—è —Å–∞–º–º–∞—Ä–∏ —Å—Ä–∞–∑—É –Ω–∞ —Ü–µ–ª–µ–≤—ã—Ö —è–∑—ã–∫–∞—Ö
			if daSum, err := translate.SummarizeText(n.Content, "da"); err == nil && strings.TrimSpace(daSum) != "" {
				n.SummaryDanish = daSum
			} else {
				n.SummaryDanish = fallbackSummary(n.Content)
			}
			if ukSum, err := translate.SummarizeText(n.Content, "uk"); err == nil && strings.TrimSpace(ukSum) != "" {
				n.SummaryUkrainian = ukSum
			} else {
				n.SummaryUkrainian = fallbackSummary(n.Content)
			}

			// –£–∫—Ä–∞–∏–Ω—Å–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
			if ukTitle, err := translate.TranslateText(n.Title, sourceLang, "uk"); err == nil && strings.TrimSpace(ukTitle) != "" {
				n.TitleUkrainian = ukTitle
			}

		} else {
			aiResp, err := aiClient.TranslateAndSummarizeNews(n.Title, n.Content)
			if err != nil {
				log.Printf("‚ö†Ô∏è Gemini failed: %v, trying fallback AI services", err)

				// Gemini –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª ‚Äî –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ AI —Å–∞–º–º–∞—Ä–∏
				n.Summary = fallbackSummary(n.Content)
				if ukSum, err := translate.SummarizeText(n.Content, "uk"); err == nil && strings.TrimSpace(ukSum) != "" {
					n.SummaryUkrainian = ukSum
				} else {
					n.SummaryUkrainian = fallbackSummary(n.Content)
				}
				if daSum, err := translate.SummarizeText(n.Content, "da"); err == nil && strings.TrimSpace(daSum) != "" {
					n.SummaryDanish = daSum
				} else {
					n.SummaryDanish = fallbackSummary(n.Content)
				}
				if ukTitle, err := translate.TranslateText(n.Title, sourceLang, "uk"); err == nil && strings.TrimSpace(ukTitle) != "" {
					n.TitleUkrainian = ukTitle
				}
			} else {
				// Gemini —É—Å–ø–µ—à–Ω–æ
				n.Summary = aiResp.Summary
				n.SummaryDanish = aiResp.Danish
				n.SummaryUkrainian = aiResp.Ukrainian
				if ukTitle, err := translate.TranslateText(n.Title, sourceLang, "uk"); err == nil && strings.TrimSpace(ukTitle) != "" {
					n.TitleUkrainian = ukTitle
				}
				log.Printf("‚úÖ Gemini translation successful")
			}
			geminiRequests++
		}
		res = append(res, n)
		time.Sleep(1 * time.Second) // –£–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
	}

	log.Printf("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ %d –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π", len(res))
	return res, nil
}

func fallbackSummary(content string) string {
	c := strings.TrimSpace(content)
	if c == "" {
		return "(–ù–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞)"
	}
	sentences := strings.Split(c, ".")
	var picked []string
	for _, s := range sentences {
		s = strings.TrimSpace(s)
		if len(s) < 25 {
			continue
		}
		picked = append(picked, s)
		if len(picked) >= 2 {
			break
		}
	}
	if len(picked) == 0 {
		if len(c) > 160 {
			return c[:160] + "..."
		}
		return c
	}
	return strings.Join(picked, ". ") + "."
}

// FormatNews produces concise formatted output with summaries.
func FormatNews(n News) string {
	var b strings.Builder
	b.WriteString("üá©üá∞ *" + n.Title + "*\n")
	if n.SummaryUkrainian != "" {
		b.WriteString("üá∫üá¶ " + n.SummaryUkrainian + "\n")
	}
	if n.SummaryDanish != "" {
		b.WriteString("üá©üá∞ " + n.SummaryDanish + "\n")
	}
	b.WriteString("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
	return b.String()
}

// FormatNewsWithImage —Å–æ–∑–¥–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Ç–æ—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –∏–∑ –¢–ó (–±–µ–∑ HTML —Ä–∞–∑–º–µ—Ç–∫–∏)
func FormatNewsWithImage(n News) string {
	var b strings.Builder
	b.WriteString("üá©üá∞ Danish News üá∫üá¶\n")
	b.WriteString("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n")

	// –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é, —á—Ç–æ–±—ã Telegram –º–æ–≥ —Å–¥–µ–ª–∞—Ç—å –ø—Ä–µ–≤—å—é (–µ—Å–ª–∏ —Ñ–æ—Ç–æ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ)
	if strings.TrimSpace(n.Link) != "" {
		b.WriteString(n.Link + "\n\n")
	}

	// –î–∞—Ç—Å–∫–∏–π –±–ª–æ–∫
	daTitle := n.Title
	if strings.TrimSpace(n.SummaryDanish) == "" {
		// –ï—Å–ª–∏ –¥–∞—Ç—Å–∫–æ–≥–æ –Ω–µ—Ç ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ–æ–ª–±—ç–∫ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
		n.SummaryDanish = fallbackSummary(n.Content)
	}
	b.WriteString("üá©üá∞ " + daTitle + "\n")
	b.WriteString(n.SummaryDanish + "\n\n")

	// –£–∫—Ä–∞–∏–Ω—Å–∫–∏–π –±–ª–æ–∫
	ukTitle := n.TitleUkrainian
	if strings.TrimSpace(ukTitle) == "" {
		ukTitle = n.Title // —Ñ–æ–ª–±—ç–∫
	}
	ukText := n.SummaryUkrainian
	if strings.TrimSpace(ukText) == "" {
		ukText = fallbackSummary(n.Content)
	}
	b.WriteString("üá∫üá¶ " + ukTitle + "\n")
	b.WriteString(ukText + "\n\n")

	b.WriteString("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n")
	b.WriteString("üì± Danish News Bot - DeusFlow")
	return b.String()
}

// trimToWordBoundary trims string to <= max, cutting at last space and adding ellipsis if trimmed.
func trimToWordBoundary(s string, max int) string {
	s = strings.TrimSpace(s)
	if max <= 0 || utf8.RuneCountInString(s) <= max {
		return s
	}
	runes := []rune(s)
	if len(runes) <= max {
		return s
	}
	cutRunes := runes[:max]
	cutStr := string(cutRunes)
	if i := strings.LastIndex(cutStr, " "); i >= 0 && utf8.RuneCountInString(cutStr)-utf8.RuneCountInString(cutStr[:i]) <= 50 {
		cutStr = strings.TrimSpace(cutStr[:i])
	} else {
		cutStr = strings.TrimSpace(cutStr)
	}
	if cutStr == "" {
		return string(cutRunes)
	}
	return cutStr + "..."
}

// FormatCaptionForPhoto builds a compact, bilingual caption that fits into maxLen (<=1024 for Telegram photo captions).
func FormatCaptionForPhoto(n News, maxLen int) string {
	if maxLen <= 0 || maxLen > 1024 {
		maxLen = 1024
	}
	// Prepare pieces
	daTitle := strings.TrimSpace(n.Title)
	ukTitle := strings.TrimSpace(n.TitleUkrainian)
	if ukTitle == "" {
		ukTitle = daTitle
	}
	daSum := strings.TrimSpace(n.SummaryDanish)
	if daSum == "" {
		daSum = fallbackSummary(n.Content)
	}
	ukSum := strings.TrimSpace(n.SummaryUkrainian)
	if ukSum == "" {
		ukSum = fallbackSummary(n.Content)
	}
	// Condense to at most two sentences for photo caption
	daSum = condenseSummary(daSum, 2)
	ukSum = condenseSummary(ukSum, 2)

	// Static header and separators (shorter for photo caption)
	header := "üá©üá∞ Danish News üá∫üá¶\n\n"
	footer := ""

	// Skeleton without summaries to measure base (rune-aware)
	composeBase := func(daT, ukT string) string {
		var b strings.Builder
		b.WriteString(header)
		b.WriteString("üá©üá∞ " + daT + "\n")
		b.WriteString("%DA%\n\n")
		b.WriteString("üá∫üá¶ " + ukT + "\n")
		b.WriteString("%UK%\n\n")
		b.WriteString(footer)
		return b.String()
	}

	capStr := composeBase(daTitle, ukTitle)
	baseLen := utf8.RuneCountInString(strings.ReplaceAll(strings.ReplaceAll(capStr, "%DA%", ""), "%UK%", ""))
	// If even titles + header/footer exceed limit, trim titles first
	if baseLen >= maxLen-40 { // leave minimal budget for summaries
		roomForTitles := maxLen - utf8.RuneCountInString(header) - utf8.RuneCountInString(footer) - 8 - 40
		if roomForTitles < 20 {
			roomForTitles = 20
		}
		each := roomForTitles / 2
		daTitle = trimToWordBoundary(daTitle, each)
		ukTitle = trimToWordBoundary(ukTitle, each)
		capStr = composeBase(daTitle, ukTitle)
		baseLen = utf8.RuneCountInString(strings.ReplaceAll(strings.ReplaceAll(capStr, "%DA%", ""), "%UK%", ""))
	}

	available := maxLen - baseLen
	if available < 40 {
		available = 40
	}
	// Dynamic allocation: minimal floor for each, remainder proportional to lengths
	minFloor := available / 5 // 20% floor split
	if minFloor < 100 {
		minFloor = 100
	}
	rem := available - 2*minFloor
	if rem < 0 {
		rem = 0
	}
	daLen := utf8.RuneCountInString(daSum)
	ukLen := utf8.RuneCountInString(ukSum)
	totalLen := daLen + ukLen
	var daBudget, ukBudget int
	if totalLen > 0 && rem > 0 {
		daBudget = minFloor + rem*daLen/totalLen
		ukBudget = minFloor + rem*ukLen/totalLen
	} else {
		daBudget = available / 2
		ukBudget = available - daBudget
	}

	daSum = trimToWordBoundary(daSum, daBudget)
	ukSum = trimToWordBoundary(ukSum, ukBudget)

	caption := strings.Replace(capStr, "%DA%", daSum, 1)
	caption = strings.Replace(caption, "%UK%", ukSum, 1)

	// Final guard rune-aware
	if utf8.RuneCountInString(caption) > maxLen {
		r := []rune(caption)
		caption = string(r[:maxLen-1]) + "‚Ä¶"
	}
	return caption
}

// condenseSummary returns up to maxSentences sentences from s, trimmed and joined with proper punctuation.
func condenseSummary(s string, maxSentences int) string {
	s = strings.TrimSpace(s)
	if s == "" || maxSentences <= 0 {
		return s
	}
	// naive sentence split on . ! ? keeping Unicode letters
	seps := []rune{'.', '!', '?'}
	var sentences []string
	var cur []rune
	for _, r := range []rune(s) {
		cur = append(cur, r)
		for _, sep := range seps {
			if r == sep {
				str := strings.TrimSpace(string(cur))
				if len([]rune(str)) >= 15 { // skip too short fragments
					sentences = append(sentences, str)
				}
				cur = cur[:0]
				break
			}
		}
		if len(sentences) >= maxSentences {
			break
		}
	}
	if len(sentences) == 0 {
		// fallback: first ~2 chunks by naive split
		parts := strings.Split(s, ".")
		for _, p := range parts {
			p = strings.TrimSpace(p)
			if p == "" {
				continue
			}
			sentences = append(sentences, p+".")
			if len(sentences) >= maxSentences {
				break
			}
		}
	}
	res := strings.Join(sentences, " ")
	return strings.TrimSpace(res)
}

// normalizeURL —É–¥–∞–ª—è–µ—Ç —Ç—Ä–µ–∫–∏–Ω–≥–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç host/path –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
func normalizeURL(raw string) string {
	if raw == "" {
		return ""
	}
	u, err := url.Parse(raw)
	if err != nil || u.Scheme == "" {
		// –ø–æ–ø—ã—Ç–∫–∞ –¥–æ–±–∞–≤–∏—Ç—å —Å—Ö–µ–º—É
		u, err = url.Parse("https://" + raw)
		if err != nil {
			return strings.ToLower(strings.TrimSpace(raw))
		}
	}
	u.Fragment = ""
	// —É–¥–∞–ª—è–µ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ç—Ä–µ–∫–∏–Ω–≥–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
	q := u.Query()
	for _, p := range []string{"utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content", "fbclid", "gclid"} {
		q.Del(p)
	}
	u.RawQuery = q.Encode()
	host := strings.ToLower(u.Host)
	host = strings.TrimPrefix(host, "www.")
	u.Host = host
	// —Å—Ö–ª–æ–ø—ã–≤–∞–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ —Å–ª–µ—à–∏ –∏ —É–±–∏—Ä–∞–µ–º –∑–∞–≤–µ—Ä—à–∞—é—â–∏–π —Å–ª–µ—à
	u.Path = strings.TrimRight(regexp.MustCompile(`/+`).ReplaceAllString(u.Path, "/"), "/")
	return u.Scheme + "://" + u.Host + u.Path + func() string {
		if u.RawQuery == "" {
			return ""
		}
		return "?" + u.RawQuery
	}()
}

// shingleSet –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç k-–≥—Ä–∞–º–Ω—ã–µ —à–∏–Ω–≥–ª—ã –¥–ª—è —Å—Ç—Ä–æ–∫–∏ s (–Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, –±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏)
func shingleSet(s string, k int) map[string]struct{} {
	s = strings.ToLower(s)
	// –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã/–ø—Ä–æ–±–µ–ª—ã
	re := regexp.MustCompile(`[^[:alnum:]\s]+`)
	s = re.ReplaceAllString(s, " ")
	words := strings.Fields(s)
	out := make(map[string]struct{})
	if len(words) == 0 {
		return out
	}
	for i := 0; i <= len(words)-k; i++ {
		sh := strings.Join(words[i:i+k], " ")
		out[sh] = struct{}{}
	}
	// —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
	if len(out) == 0 {
		for _, w := range words {
			out[w] = struct{}{}
		}
	}
	return out
}

// jaccardSimilarity –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–æ–∫–∞–º–∏ –∏—Å–ø–æ–ª—å–∑—É—è k-–≥—Ä–∞–º–Ω—ã–µ —à–∏–Ω–≥–ª—ã
func jaccardSimilarity(a, b string, k int) float64 {
	sa := shingleSet(a, k)
	sb := shingleSet(b, k)
	if len(sa) == 0 || len(sb) == 0 {
		return 0.0
	}
	inter := 0
	for sh := range sa {
		if _, ok := sb[sh]; ok {
			inter++
		}
	}
	union := len(sa) + len(sb) - inter
	if union == 0 {
		return 0.0
	}
	return float64(inter) / float64(union)
}

// isSimilarTitle –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç true –µ—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —è–≤–ª—è—é—Ç—Å—è –±–ª–∏–∑–∫–∏–º–∏ –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏ (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π –ø–æ—Ä–æ–≥)
func isSimilarTitle(a, b string) bool {
	// –∏—Å–ø–æ–ª—å–∑—É–µ–º 2-–≥—Ä–∞–º–Ω—ã–µ —à–∏–Ω–≥–ª—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤; –ø–æ—Ä–æ–≥ = 0.55
	if a == "" || b == "" {
		return false
	}
	score := jaccardSimilarity(a, b, 2)
	return score >= 0.55
}

// selectDiverse –≤—ã–±–∏—Ä–∞–µ—Ç –¥–æ limit —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–∑ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö candidates —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
// candidates –æ–∂–∏–¥–∞–µ—Ç—Å—è –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ score desc + recency
func selectDiverse(candidates []News, limit int, perSource int, perCategory int) []News {
	if limit <= 0 {
		return nil
	}
	out := make([]News, 0, limit)
	srcCount := make(map[string]int)
	catCount := make(map[string]int)

	// –ø—Ä–æ–±—É–µ–º –∂–∞–¥–Ω—ã–π –≤—ã–±–æ—Ä; –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å–º—è–≥—á–∞–µ–º –∫–≤–æ—Ç—ã –≤–æ –≤—Ç–æ—Ä–æ–º –ø—Ä–æ—Ö–æ–¥–µ
	for _, c := range candidates {
		if len(out) >= limit {
			break
		}
		if c.Link == "" {
			continue
		}
		if perSource > 0 && srcCount[c.SourceName] >= perSource {
			continue
		}
		if perCategory > 0 && catCount[c.Category] >= perCategory {
			continue
		}
		out = append(out, c)
		srcCount[c.SourceName]++
		catCount[c.Category]++
	}

	// –µ—Å–ª–∏ –Ω–µ –∑–∞–ø–æ–ª–Ω–∏–ª–∏, –∑–∞–ø–æ–ª–Ω—è–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è perSource/perCategory –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∫–≤–æ—Ç—ã
	if len(out) < limit {
		for _, c := range candidates {
			if len(out) >= limit {
				break
			}
			already := false
			for _, x := range out {
				if x.Link == c.Link {
					already = true
					break
				}
			}
			if already {
				continue
			}
			out = append(out, c)
		}
	}

	// —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ (score desc)
	sort.SliceStable(out, func(i, j int) bool {
		if out[i].Score != out[j].Score {
			return out[i].Score > out[j].Score
		}
		return out[i].Published.After(out[j].Published)
	})
	return out
}

// extractImageURL –∏–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ RSS —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–ª–∏ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
func extractImageURL(item *rss.FeedItem) string {
	// 1) –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ enclosures –∏–∑ RSS (gofeed –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç item.Enclosures)
	if item.Enclosures != nil {
		for _, e := range item.Enclosures {
			if e == nil {
				continue
			}
			// –µ—Å–ª–∏ —Ç–∏–ø —è–≤–Ω–æ image/* ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º
			if strings.HasPrefix(strings.ToLower(e.Type), "image/") && strings.TrimSpace(e.URL) != "" {
				return e.URL
			}
			// –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∏–¥—ã —É–∫–∞–∑—ã–≤–∞—é—Ç —Ç–æ–ª—å–∫–æ URL –±–µ–∑ type
			if strings.TrimSpace(e.URL) != "" && (strings.HasSuffix(strings.ToLower(e.URL), ".jpg") || strings.HasSuffix(strings.ToLower(e.URL), ".jpeg") || strings.HasSuffix(strings.ToLower(e.URL), ".png") || strings.HasSuffix(strings.ToLower(e.URL), ".webp") || strings.HasSuffix(strings.ToLower(e.URL), ".gif")) {
				return e.URL
			}
		}
	}

	// 2) –ü–æ–∏—Å–∫ <img src> –≤ Description
	if item.Description != "" {
		imgRe := regexp.MustCompile(`<img[^>]+src=["']([^"']+)["'][^>]*>`)
		if m := imgRe.FindStringSubmatch(item.Description); len(m) > 1 {
			return m[1]
		}
	}

	// 3) –ü–æ–∏—Å–∫ <img src> –≤ Content (–µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ñ–∏–¥–µ –±–æ–≥–∞—á–µ)
	if item.Content != "" {
		imgRe := regexp.MustCompile(`<img[^>]+src=["']([^"']+)["'][^>]*>`)
		if m := imgRe.FindStringSubmatch(item.Content); len(m) > 1 {
			return m[1]
		}
	}

	// 4) Fallback: fetch og:image from page
	if strings.TrimSpace(item.Link) != "" {
		if og, err := scraper.ExtractImageURL(item.Link); err == nil && strings.TrimSpace(og) != "" {
			return og
		}
	}

	return ""
}
